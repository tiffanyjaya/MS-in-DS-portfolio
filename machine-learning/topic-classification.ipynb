{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=False,\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=False,\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[int(num_test/2):], newsgroups_test.target[int(num_test/2):]\n",
    "dev_data, dev_labels = newsgroups_test.data[:int(num_test/2)], newsgroups_test.target[:int(num_test/2)]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label.\n",
    "\n",
    "[2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Example: 1\n",
      "Newsgroup: label 0, alt.atheism\n",
      "Message:\n",
      "\n",
      "\n",
      "I think that domestication will change behavior to a large degree.\n",
      "Domesticated animals exhibit behaviors not found in the wild.  I\n",
      "don't think that they can be viewed as good representatives of the\n",
      "wild animal kingdom, since they have been bred for thousands of years\n",
      "to produce certain behaviors, etc.\n",
      "--------------------------------------------------------------------------------\n",
      "Example: 2\n",
      "Newsgroup: label 2, sci.space\n",
      "Message:\n",
      "\n",
      "\n",
      "I think I can. Largely as a result of efforts by people reading this group\n",
      "writing letters and making phone calls the following has happened:\n",
      "\n",
      "1. NASA reprogrammed funds to keep NASP alive in 1991.\n",
      "2. Efforts to kill DC-X and the SSRT progam where twice twarted\n",
      "   (Feb. and June of last year).\n",
      "3. Gouldin kept his job in spite of heavy lobbying against him.\n",
      "\n",
      "This may not be what Mark was thinking of but it shows that the\n",
      "readers of sci.space DO have power and influence.\n",
      "\n",
      "  Allen\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Example: 3\n",
      "Newsgroup: label 1, comp.graphics\n",
      "Message:\n",
      "In regards to fractal commpression, I have seen 2 fractal compressed \"movies\".\n",
      "They were both fairly impressive.  The first one was a 64 gray scale \"movie\" of\n",
      "Casablanca, it was 1.3MB and had 11 minutes of 13 fps video.  It was a little\n",
      "grainy but not bad at all.  The second one I saw was only 3 minutes but it\n",
      "had 8 bit color with 10fps and measured in at 1.2MB.\n",
      "\n",
      "I consider the fractal movies a practical thing to explore.  But unlike many \n",
      "other formats out there, you do end up losing resolution.  I don't know what\n",
      "kind of software/hardware was used for creating the \"movies\" I saw but the guy\n",
      "that showed them to me said it took 5-15 minutes per frame to generate.  But as\n",
      "I said above playback was 10 or more frames per second.  And how else could you\n",
      "put 11 minutes on one floppy disk?\n",
      "--------------------------------------------------------------------------------\n",
      "Example: 4\n",
      "Newsgroup: label 0, alt.atheism\n",
      "Message:\n",
      " \n",
      " \n",
      "(Deletion)\n",
      " \n",
      " \n",
      "You have given that example. It is not lenient. End of argument.\n",
      " \n",
      "And chopping off the hands or heads of people is not lenient either. It\n",
      "rather appears that you are internalized the claims about the legal system\n",
      "without checking if they suit the description.\n",
      " \n",
      "And wasn't the argument that it takes five men to rape a woman according\n",
      "to Islamic law?\n",
      " \n",
      " \n",
      " \n",
      "No, I even believe what I don't like. Can you give better answers than that?\n",
      "Have you got any evidence for your probably opposite claims?\n",
      " \n",
      " \n",
      " \n",
      "A fact, if memory serves. And most will see the connection between the\n",
      "primitive machism in the Orient and in Islam.\n",
      " \n",
      " \n",
      "As usually you miss the point. Aids is  neither spread only through sex\n",
      "nor necessarily spread by having sex. Futher, the point is, a very important\n",
      "point, the urge for sex is stronger than the fear of AIDS. It is even\n",
      "stronger than the religious attempts to channel or to forbid sex. The\n",
      "consequences of suppressing sex are worse than the consequences of Aids.\n",
      "Please note that the idea that everybody would end up with AIDS when sex\n",
      "is not controlled is completely counterfactual.\n",
      " \n",
      "--------------------------------------------------------------------------------\n",
      "Example: 5\n",
      "Newsgroup: label 2, sci.space\n",
      "Message:\n",
      "Background: The Orion spacedrive was a theoretical concept.  It\n",
      "would be a drive using thermonuclear explosions to drive a spacecraft.\n",
      "The idea was that you'd detonate devices with somewhere from one to\n",
      "ten megatons yield behind a \"pusher plate\" attached to the main\n",
      "spacecraft.  The shock wave from the explosions would transfer\n",
      "momentum to the ship.\n",
      "\n",
      "  Now, in an atmosphere I can see this.  The energy of the explosion\n",
      "heats the atmosphere, which expands explosively and slams a shock wave\n",
      "into the pusher plate.  But in a vacuum, only two things I can see are\n",
      "going to hit the plate: fission/fusion products (barium, krypton,\n",
      "helium, neutrons, evaporated bomb casing) and electromagnetic\n",
      "radiation (gammas mostly, some light/heat from irradiated fission\n",
      "products).\n",
      "\n",
      "  Would this work?  I can't see the EM radiation impelling very much\n",
      "momentum (especially given the mass of the pusher plate), and it seems\n",
      "to me you're going to get more momentum transfer throwing the bombs\n",
      "out the back of the ship than you get from detonating them once\n",
      "they're there.\n",
      "\n",
      "  I must be missing something.  Would someone enlighten me via email?\n",
      "\n",
      "  Thanks.\n",
      "\n",
      "-- \n",
      "\t--Jim\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "### STUDENT START ###\n",
    "    for i in range(num_examples):\n",
    "        print('-' * 80)\n",
    "        print('Example: %d' % (i + 1))\n",
    "        print('Newsgroup: label %d, %s' % (train_labels[i], newsgroups_train.target_names[train_labels[i]]))\n",
    "        print('Message:\\n%s' % (train_data[i]))\n",
    "### STUDENT END ###\n",
    "\n",
    "# also passed in the function globally: \n",
    "# train_data, train_labels, newsgroups_train.target_names\n",
    "P1(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "\n",
    "[6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "\n",
      "2.a.\n",
      "What is the size of the vocabulary? 26879\n",
      "What is the average number of non-zero features per example? 96.7060\n",
      "What fraction of the entires in the matrix are non-zero? 0.0036\n",
      "\n",
      "2.b.\n",
      "What are the 0th feature string (in alphabetical order)? 00\n",
      "What are the last feature string (in alphabetical order)? zyxel\n",
      "\n",
      "2.c.\n",
      "Specify your own vocabulary with 4 words: ['atheism', 'graphics', 'space', 'religion']\n",
      "Confirm the training vectors are appropraitely shaped. I expect it to be (2034, 4): (2034, 4)\n",
      "What is the average number of non-zero features per example? 0.2684\n",
      "\n",
      "2.d.\n",
      "What size vocabulary does this (extracting bigram and trigram character features) yield? 35478\n",
      "\n",
      "2.e.\n",
      "What size vocabulary does this (pruning words that appear in fewer than 10 documents) yield? 3064\n",
      "\n",
      "2.f.\n",
      "What fraction of the words in the dev data are missing from the vocabulary?\n",
      "a. Out of dev data: 0.2489\n",
      "b. Out of train data: 0.1352\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "### STUDENT START ###\n",
    "\n",
    "    print('ANSWER:\\n')\n",
    "\n",
    "    # create the vectorizer that transforms text into term-document matrix \n",
    "    # with row = # of messages (or examples), column = # of vocabularies, value = # of occurences\n",
    "    vect = CountVectorizer()\n",
    "    # fit: learn the vocabulary dictionary (aka raw training text)\n",
    "    # transform: return term-document matrix (aka feature vectors)\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "\n",
    "    print('2.a.')\n",
    "    # train_features.shape = (# of messages, # of vocabs)\n",
    "    num_vocabs = train_features.shape[1]\n",
    "    #num_vocabs = len(vect.vocabulary_.values())\n",
    "    num_messages = train_features.shape[0]\n",
    "    # number of nonzero elements in term-document matrix (aka feature vectors)\n",
    "    num_nonzero_features = train_features.nnz\n",
    "    print('What is the size of the vocabulary? %d' % \n",
    "          num_vocabs)\n",
    "    print('What is the average number of non-zero features per example? %.4f' % \n",
    "          round(num_nonzero_features/num_messages, 4))\n",
    "    print('What fraction of the entires in the matrix are non-zero? %.4f' % \n",
    "          round(num_nonzero_features/(num_messages * num_vocabs), 4))\n",
    "\n",
    "    print('\\n2.b.')\n",
    "    train_feature_names = vect.get_feature_names()\n",
    "    print('What are the 0th feature string (in alphabetical order)? %s' % \n",
    "         vect.get_feature_names()[0])\n",
    "    print('What are the last feature string (in alphabetical order)? %s' %\n",
    "         vect.get_feature_names()[-1])\n",
    "    \n",
    "    print('\\n2.c.')\n",
    "    # ignore words that do not appear in the provided list of words (aka myvocab)\n",
    "    myvocab = ['atheism', 'graphics', 'space', 'religion']\n",
    "    vect = CountVectorizer(vocabulary = myvocab)\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    print('Specify your own vocabulary with 4 words: {}'.format(myvocab))\n",
    "    print('Confirm the training vectors are appropraitely shaped. I expect it to be ({}, {}): {}'.format(\n",
    "        num_messages, len(myvocab), train_features.shape))\n",
    "    print('What is the average number of non-zero features per example? %.4f' % \n",
    "          round(train_features.nnz/train_features.shape[0], 4))\n",
    "    \n",
    "    print('\\n2.d.')\n",
    "    vect = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    print('What size vocabulary does this (extracting bigram and trigram character features) yield? %d' %\n",
    "         train_features.shape[1])\n",
    "    \n",
    "    print('\\n2.e.')\n",
    "    # minimum document frequency\n",
    "    min_df = 10\n",
    "    vect = CountVectorizer(min_df = min_df)\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    print('What size vocabulary does this (pruning words that appear in fewer than 10 documents) yield? %d' %\n",
    "         train_features.shape[1])\n",
    "    \n",
    "    print('\\n2.f.')\n",
    "    vect = CountVectorizer()\n",
    "    dev_features = vect.fit_transform(dev_data)\n",
    "    dev_feature_names = vect.get_feature_names()\n",
    "    print('What fraction of the words in the dev data are missing from the vocabulary?\\n' + \n",
    "          'a. Out of dev data: {:.4f}\\nb. Out of train data: {:.4f}'.format(\n",
    "        len(set(dev_feature_names) - set(train_feature_names)) / len(dev_feature_names),\n",
    "        len(set(dev_feature_names) - set(train_feature_names)) / len(train_feature_names)))\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "# also passed in the function globally: \n",
    "# train_data, dev_data\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Model type: <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "k: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49]\n",
      "optimal k: 17\n",
      "F1-score for optimal k: 0.4180\n",
      "--------------------------------------------------------------------------------\n",
      "Model type: <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "alpha: [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n",
      "optimal alpha: 0.1\n",
      "F1-score for optimal alpha: 0.7712\n",
      "--------------------------------------------------------------------------------\n",
      "Model type: <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "C: [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n",
      "optimal C: 0.4\n",
      "F1-score for optimal C: 0.7230\n",
      "sum of the squared weight values for each class for each setting of the C parameter:\n",
      "C: 0.1, sum of squared weight values: [27.13199955 24.65957471 27.4587074  23.02184411]\n",
      "C: 0.2, sum of squared weight values: [49.72936301 42.74921294 49.32793815 42.6400772 ]\n",
      "C: 0.3, sum of squared weight values: [69.28068753 57.87662416 67.91691857 59.75870012]\n",
      "C: 0.4, sum of squared weight values: [86.73338222 71.14481799 84.26622373 75.05776817]\n",
      "C: 0.5, sum of squared weight values: [102.6092906   83.11440941  99.02539442  89.00695691]\n",
      "C: 0.6, sum of squared weight values: [117.1848553   94.03395125 112.50558729 101.84840473]\n",
      "C: 0.7, sum of squared weight values: [130.86787517 104.14840407 125.04723727 113.8500424 ]\n",
      "C: 0.8, sum of squared weight values: [143.60098248 113.58482533 136.6779247  125.03715031]\n",
      "C: 0.9, sum of squared weight values: [155.53608773 122.45506662 147.6449077  135.75183599]\n"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "### STUDENT START ###\n",
    "    \n",
    "    # define changing values\n",
    "    # 1. for knn, generate a list of odd k's ranging from 1 to 50\n",
    "    n_neighbors = list(range(1, 50, 2))\n",
    "    # 2. for multinomial naive bayes, a list of alpha's ranging from 0 to 0.1\n",
    "    alphas = np.arange(0.1, 1, 0.1)\n",
    "    # 3. for logistic regression, a list of C's ranging from 0.1 to 1 (C > 0)\n",
    "    Cs = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "    # create the vectorizer that transforms text into term-document matrix (aka features)\n",
    "    # with row = # of messages (or examples), column = # of vocabularies, value = # of occurences\n",
    "    vect = CountVectorizer()\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    dev_features = vect.transform(dev_data)\n",
    "        \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # k nearest neighbors: find the optimal value for k\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    # perform cross validation to find the best params for the model\n",
    "    knn_cv = GridSearchCV(knn_model, {'n_neighbors': n_neighbors}, scoring='f1_weighted')\n",
    "    knn_cv.fit(train_features, train_labels)\n",
    "    print('-' * 80)\n",
    "    print('Model type: {}'.format(type(knn_model)))\n",
    "    print('k: {}'.format(n_neighbors))\n",
    "    print('optimal k: {}'.format(knn_cv.best_params_['n_neighbors']))\n",
    "    print('F1-score for optimal k: {:.4f}'.format(\n",
    "        metrics.f1_score(dev_labels, knn_cv.predict(dev_features), average='weighted')))\n",
    "    # note: knn_cv.best_estimator_.predict == knn_cv.predict\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # multinomial naive bayes: find the optimal value for alpha\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    mnb_model = MultinomialNB()\n",
    "    # perform cross validation to find the best params for the model\n",
    "    mnb_cv = GridSearchCV(mnb_model, {'alpha': alphas}, scoring='f1_weighted')\n",
    "    mnb_cv.fit(train_features, train_labels)\n",
    "    print('-' * 80)\n",
    "    print('Model type: {}'.format(type(mnb_model)))\n",
    "    print('alpha: {}'.format(alphas))\n",
    "    print('optimal alpha: {}'.format(mnb_cv.best_params_['alpha']))\n",
    "    print('F1-score for optimal alpha: {:.4f}'.format(\n",
    "        metrics.f1_score(dev_labels, mnb_cv.predict(dev_features), average='weighted')))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # logistic regression: find the optimal value for the regularization strength C using l2 regularization\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    lr_model = LogisticRegression()\n",
    "    # perform cross validation to find the best params for the model\n",
    "    lr_cv = GridSearchCV(lr_model, {'penalty': ['l2'], 'C': Cs}, scoring='f1_weighted')\n",
    "    lr_cv.fit(train_features, train_labels)\n",
    "    print('-' * 80)\n",
    "    print('Model type: {}'.format(type(lr_model)))\n",
    "    print('C: {}'.format(Cs))\n",
    "    print('optimal C: {}'.format(lr_cv.best_params_['C']))\n",
    "    print('F1-score for optimal C: {:.4f}'.format(\n",
    "        metrics.f1_score(dev_labels, lr_cv.predict(dev_features), average='weighted')))\n",
    "    \n",
    "    print('sum of the squared weight values for each class for each setting of the C parameter:')\n",
    "    for C in Cs:\n",
    "        model = LogisticRegression(C=C) # defaults to penalty = 'l2'\n",
    "        model.fit(train_features, train_labels)\n",
    "        print('C: {}, sum of squared weight values: {}'.format(\n",
    "            round(C, 2), str(np.sum(np.square(model.coef_), axis=1))))\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "### STUDENT END ###\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: \n",
    "\n",
    "3a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "Nearest neighbor does not work well for this problem because of the curse of dimensionality. It is difficult for nearest neighbor to classify multiple features (approximately 26879 features). \n",
    "\n",
    "3b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "Logistic regression does not work as well as Naive Bayes because:\n",
    "\n",
    "* The assumption for logistic regression is not met while the assumption for Naive Bayes is. Logistic regression assumes that all observations are independent when this is not the case. One message might be correlated with another message because they are posted within the same newsgroup. On the other hand, Naive Bayes assumes that all features are conditionally independent. This assumption is mostly met because the probability of a word appearing in one newsgroup is most likely not related to the probability of the same word appearing in another newsgroup. The only exception to this might be newsgroups which are similarly related to each other like \"atheism\" and \"religion.\"\n",
    "* The training dataset is small, which can lead logistic regression to overfitting. On the other hand, Naive Bayes will not have this issue because it estimates based on the joint density function. \n",
    "\n",
    "3c. For logistic regression, briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "As can be seen from the outputs, larger C corresponds to larger values of sum of squared weights. C is the inverse of regularization strength, which means larger values of C correspond to weaker regularization. Smaller values of C corresponds to stronger regularization. We add regularization term to the cost function in order to shrink the parameters so that we do not overfit the model onto the data. We do not want to penalize and shrink the parameters too much that we underfit the data. In summary, smaller C corresponds to stronger regularization which in turn leads to smaller values of the sum of squared weights that shrink the parameters to prevent the model from overfitting the data. \n",
    "\n",
    "https://www.youtube.com/watch?v=KvtGD37Rm5I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?\n",
    "\n",
    "[5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "                                       1-gram\n",
      "--------------------------------------------------------------------------------\n",
      "        vocab  alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "1    atheists     0.774253      -0.092135  -0.250866           -0.579621\n",
      "1       bobby     0.772306      -0.181621  -0.269023           -0.363592\n",
      "1    religion     0.751261      -0.475539  -0.618348           -0.043954\n",
      "1     atheism     0.748960      -0.330803  -0.341601           -0.360325\n",
      "1       islam     0.637761      -0.101141  -0.266972           -0.270773\n",
      "2    graphics    -0.605578       1.531430  -1.032555           -0.586016\n",
      "2       image    -0.435731       1.045629  -0.624949           -0.354875\n",
      "2        file    -0.258699       0.996074  -0.644018           -0.471149\n",
      "2          3d    -0.285813       0.884192  -0.531064           -0.297957\n",
      "2    computer     0.050117       0.803401  -0.533933           -0.369460\n",
      "3       space    -0.999749      -1.054570   1.773219           -0.912475\n",
      "3       orbit    -0.330445      -0.525121   0.948498           -0.459199\n",
      "3        nasa    -0.433666      -0.388352   0.803482           -0.380446\n",
      "3      launch    -0.349890      -0.374896   0.737214           -0.264350\n",
      "3  spacecraft    -0.279020      -0.300534   0.682052           -0.266190\n",
      "4   christian    -0.446429      -0.315491  -0.241389            0.866855\n",
      "4  christians    -0.565958      -0.289170  -0.370102            0.862380\n",
      "4       blood    -0.395688      -0.092903  -0.206668            0.783134\n",
      "4         fbi    -0.224535      -0.200432  -0.352105            0.705435\n",
      "4       order    -0.605127      -0.061213  -0.117182            0.694264\n",
      "--------------------------------------------------------------------------------\n",
      "                                       2-gram\n",
      "--------------------------------------------------------------------------------\n",
      "           vocab  alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "1     claim that     0.553536      -0.182495  -0.252014           -0.124455\n",
      "1    cheers kent     0.523179      -0.644813  -0.616215            0.510039\n",
      "1        you are     0.441530      -0.267633  -0.452486            0.035958\n",
      "1        in this     0.425406       0.016003  -0.453430           -0.109434\n",
      "1       was just     0.425337      -0.115200  -0.110978           -0.207273\n",
      "2    looking for    -0.592726       1.042529  -0.466809           -0.533918\n",
      "2     in advance    -0.433184       0.788112  -0.410735           -0.391949\n",
      "2  comp graphics    -0.268033       0.730995  -0.343024           -0.255853\n",
      "2      out there    -0.259806       0.715355  -0.448509           -0.260111\n",
      "2       is there    -0.314994       0.704646  -0.438334           -0.233689\n",
      "3      the space    -0.254111      -0.494143   0.820776           -0.258441\n",
      "3       the moon    -0.333456      -0.466262   0.792875           -0.205632\n",
      "3      sci space    -0.240406      -0.310715   0.583758           -0.205598\n",
      "3       and such    -0.192466      -0.317957   0.559308           -0.206144\n",
      "3         it was    -0.186485      -0.292561   0.497761           -0.290388\n",
      "4        the fbi    -0.121946      -0.196204  -0.275524            0.519208\n",
      "4    cheers kent     0.523179      -0.644813  -0.616215            0.510039\n",
      "4         but he    -0.171439      -0.199607  -0.125093            0.454464\n",
      "4   ignorance is    -0.139971      -0.150210  -0.121975            0.439575\n",
      "4       with you    -0.218758       0.141849  -0.336296            0.393455\n"
     ]
    }
   ],
   "source": [
    "def P4(C=0.4): \n",
    "### STUDENT START ###\n",
    "\n",
    "    # define changing variables\n",
    "    top_n = 5\n",
    "    ngram_range = (1, 2)\n",
    "\n",
    "    for ngram in ngram_range:\n",
    "        # create the vectorizer that transforms text into term-document matrix (aka features)\n",
    "        # with row = # of messages (or examples), column = # of vocabularies, value = # of occurences\n",
    "        vect = CountVectorizer(ngram_range=(ngram, ngram))\n",
    "        train_features = vect.fit_transform(train_data)\n",
    "        dev_features = vect.transform(dev_data)\n",
    "\n",
    "        # train a logistic regression model with the optimal param found in P3\n",
    "        lr_model = LogisticRegression(C=C)\n",
    "        lr_model.fit(train_features, train_labels)\n",
    "\n",
    "        # find 5 features with largest weights for each label -- 20 features in total\n",
    "        # 1. for each newsgroup, find the index of top 5 features with the largest weights in descending order\n",
    "        top_weight_indexes = np.array([np.argpartition(-row, range(top_n))[:top_n] for row in lr_model.coef_])\n",
    "        # 2. reshape 4x5 array into 1x20 array\n",
    "        # lr_model.coef_[0]: len(newsgroups_train.target_names)\n",
    "        # lr_model.coef_[1]: top_n\n",
    "        reshape_col = len(newsgroups_train.target_names) * top_n\n",
    "        reshape_top_weight_indexes = top_weight_indexes.reshape(1, reshape_col).tolist()[0]\n",
    "        # 3. rename indexes into its respective vocabulary terms\n",
    "        vocab = [vect.get_feature_names()[index] for index in reshape_top_weight_indexes]\n",
    "        index = np.repeat(range(1, len(newsgroups_train.target_names)+1), top_n)\n",
    "                \n",
    "        # create a table with 20 rows and 4 columns \n",
    "        # that shows the weight for each of these features for each of the labels\n",
    "        print('-' * 80)\n",
    "        print('{:>40}-gram'.format(ngram))\n",
    "        print('-' * 80)\n",
    "        print(pd.DataFrame({\n",
    "             'vocab': vocab,\n",
    "             newsgroups_train.target_names[0]: [lr_model.coef_[0][index] for index in reshape_top_weight_indexes],\n",
    "             newsgroups_train.target_names[1]: [lr_model.coef_[1][index] for index in reshape_top_weight_indexes],\n",
    "             newsgroups_train.target_names[2]: [lr_model.coef_[2][index] for index in reshape_top_weight_indexes],\n",
    "             newsgroups_train.target_names[3]: [lr_model.coef_[3][index] for index in reshape_top_weight_indexes]\n",
    "            },\n",
    "            columns=['vocab'] + newsgroups_train.target_names,\n",
    "            index=index))\n",
    "### STUDENT END ###\n",
    "# also passed in the function globally: \n",
    "# train_data, dev_data\n",
    "P4()\n",
    "\n",
    "#  26879: 1-gram\n",
    "# 221770: 2-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "Any surprising features in the table?\n",
    "1. The top weighted words in 1-gram are relatable to their respective newsgroup.\n",
    "2. It is interesting to note that alt.atheism and talk.religion.misc newsgroup share common words, including a surprising police term. For example, \"bobby\" is a slang term for a member of Londonâ€™s Metropolitan Police while \"fbi\" is the domestic intelligence and security service of the United States. \n",
    "3. If \"bobby\" does not refer to a police term, then it could refer to active users who sign their names after each message. If that was the case, it is interesting to note that both 1-gram and 2-gram include active users as the top 5 features with largest weights. For example, \"cheers kent\" appears highly in both alt.atheism and talk.religion.misc newsgroup.\n",
    "4. 1-gram selects words that are more closely related to the newsgroup compared to that of 2-gram. Partially, in 2-gram, we did not remove any stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tiffapedia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "--------------------------------------------------------------------------------\n",
      "Feature/Vocabulary/Dictionary size\n",
      "--------------------------------------------------------------------------------\n",
      "Empty preprocessor's vocabulary size: 33291\n",
      "Better preprocessor's vocabulary size: 40643\n",
      "Vocabulary has been reduced by: -22.0840%\n",
      "--------------------------------------------------------------------------------\n",
      "F1-score\n",
      "--------------------------------------------------------------------------------\n",
      "Empty preprocessor's f1-score: 0.7050\n",
      "Better preprocessor's f1-score: 0.7108\n",
      "F1-score improves by: 0.0058\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "### STUDENT START ###\n",
    "    # 1. standardize case with lowercase\n",
    "    s = s.lower()\n",
    "    # 2. strip leading and trailing whitespaces\n",
    "    s = s.strip()\n",
    "    # 3. replace sequences of numbers with a single token (I use 0)\n",
    "    # 3.a. find and replace arithmetic sequence\n",
    "    # TODO\n",
    "    # 3.b. find and replace geometric sequence\n",
    "    # TODO\n",
    "    # 3.c. find and replace repeating sequence of a number, i.e. 5151\n",
    "    #s = re.sub('(.+.+)(\\1)+', '*', s)\n",
    "    # NOTE: adding 3.c. does not improve the f1-score\n",
    "    # 3.d. find and replace sequence of numbers \n",
    "    #s = re.sub('\\d+', '0', s)\n",
    "    # NOTE: adding 3.d. degrades the f1-score\n",
    "    # 4. remove various other non-letter characters\n",
    "    s = re.sub('[^ a-zA-Z0-9]', '', s) # alpha-numeric\n",
    "    # NOTE: alpha-numeric improves the f1-score more than letters only\n",
    "    # separate s into individual words for stemming and stop words\n",
    "    words = s.split()\n",
    "    # 5. shorten words to their stem\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # 6. remove stop words\n",
    "    words = non_stop_words = [word for word in words if not word in set(stopwords.words(\"english\"))]\n",
    "    # combine individual words back to s\n",
    "    s = ' '.join(words)\n",
    "    return s\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "def P5(C=0.4): \n",
    "### STUDENT START ###\n",
    "\n",
    "    def get_dict_size_and_f1_score(preprocessor_type):\n",
    "        # create the vectorizer that transforms text into term-document matrix (aka features)\n",
    "        # with row = # of messages (or examples), column = # of vocabularies, value = # of occurences\n",
    "        vect = CountVectorizer(preprocessor=preprocessor_type)\n",
    "        train_features = vect.fit_transform(train_data)\n",
    "        dev_features = vect.transform(dev_data)\n",
    "        \n",
    "        # train a logistic regression model with the optimal param found in P3\n",
    "        lr_model = LogisticRegression(C=C)\n",
    "        lr_model.fit(train_features, train_labels)\n",
    "\n",
    "        # obtain f1-score\n",
    "        f1_score = metrics.f1_score(dev_labels, lr_model.predict(dev_features), average='weighted')\n",
    "                \n",
    "        return train_features.shape[1], f1_score\n",
    "    \n",
    "    ep_feature_size, ep_f1_score = get_dict_size_and_f1_score(empty_preprocessor)\n",
    "    bp_feature_size, bp_f1_score = get_dict_size_and_f1_score(better_preprocessor)\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('Feature/Vocabulary/Dictionary size')\n",
    "    print('-' * 80)\n",
    "    print('Empty preprocessor\\'s vocabulary size: %d' % ep_feature_size)\n",
    "    print('Better preprocessor\\'s vocabulary size: %d' % bp_feature_size)\n",
    "    print('Vocabulary has been reduced by: {:.4f}%'.format((ep_feature_size - bp_feature_size)/ep_feature_size * 100))\n",
    "    #print('Vocabulary has been reduced by: {:.4f}'.format((1 - bp_feature_size/ep_feature_size) * 100))\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('F1-score')\n",
    "    print('-' * 80)  \n",
    "    print('Empty preprocessor\\'s f1-score: {:.4f}'.format(ep_f1_score))\n",
    "    print('Better preprocessor\\'s f1-score: {:.4f}'.format(bp_f1_score))\n",
    "    print('F1-score improves by: {:.4f}'.format(bp_f1_score - ep_f1_score))\n",
    "        \n",
    "### STUDENT END ###\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001).\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reference: \n",
    "* L1 and L2 norm: https://www.youtube.com/watch?v=5fN2J8wYnfw&list=PLvcbYUQ5t0UFhdkiCojiFOygmbMU19BFq \n",
    "* Regularization: bias the parameter towards a particular value by adding a penalty term that encourages that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.40\n",
      "--------------------------------------------------------------------------------\n",
      "penalty L1\n",
      "--------------------------------------------------------------------------------\n",
      "(n_classes, n_features): (4, 26879)\n",
      "Number of nonzero weights: 1065\n",
      "Sparsity: 99.01%\n",
      "Score: 0.9012\n",
      "--------------------------------------------------------------------------------\n",
      "penalty L2\n",
      "--------------------------------------------------------------------------------\n",
      "(n_classes, n_features): (4, 26879)\n",
      "Number of nonzero weights: 107516\n",
      "Sparsity: 0.00%\n",
      "Score: 0.9769\n",
      "--------------------------------------------------------------------------------\n",
      "penalty L2 with reduced vocabulary\n",
      "--------------------------------------------------------------------------------\n",
      "(n_classes, n_features): (4, 632)\n",
      "Number of nonzero weights: 2528\n",
      "Sparsity: 0.00%\n",
      "Score: 0.9346\n",
      "--------------------------------------------------------------------------------\n",
      "plot accuracy of re-trained model vs vocab size\n",
      "--------------------------------------------------------------------------------\n",
      "C Value                Vocab Size             Nonzero Weights        Sparsity (%)           Score                  \n",
      "0.001                  3                      12                     0.0                    0.3829891838741396     \n",
      "0.01                   18                     72                     0.0                    0.532448377581121      \n",
      "0.1                    213                    852                    0.0                    0.8082595870206489     \n",
      "0.5                    743                    2972                   0.0                    0.9444444444444444     \n",
      "1.0                    1386                   5544                   0.0                    0.971976401179941      \n",
      "5.0                    2165                   8660                   0.0                    0.9783677482792527     \n",
      "10.0                   2634                   10536                  0.0                    0.9783677482792527     \n",
      "50.0                   5836                   23344                  0.0                    0.9773844641101278     \n",
      "100.0                  7546                   30184                  0.0                    0.9783677482792527     \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12542f9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6(C=0.4):\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    # create the vectorizer that transforms text into term-document matrix (aka features)\n",
    "    # with row = # of messages (or examples), column = # of vocabularies, value = # of occurences\n",
    "    vect = CountVectorizer()\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    dev_features = vect.transform(dev_data)\n",
    "    \n",
    "    # define: \n",
    "    # 1. C based on optimized parameter found in P3\n",
    "    # 2. relax convergence criteria for extreme values of C by setting tol = 0.01 (vs 0.001)\n",
    "    tol = 0.01\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # logistic regression with penalty l1\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    l1_lr = LogisticRegression(C=C, penalty='l1', tol=tol)\n",
    "    l1_lr.fit(train_features, train_labels)\n",
    "    sparsity_l1_lr = np.mean(l1_lr.coef_.ravel() == 0) * 100\n",
    "                \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # logistic regression with penalty l2\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    l2_lr = LogisticRegression(C=C, penalty='l2', tol=tol)\n",
    "    l2_lr.fit(train_features, train_labels)\n",
    "    sparsity_l2_lr = np.mean(l2_lr.coef_.ravel() == 0) * 100\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # logistic regression with penalty l2 and reduced vocabulary \n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    # reduced vocabulary: keep features that have at least one non-zero weight\n",
    "    nonzero_indexes = np.nonzero(np.any(l1_lr.coef_ != 0, axis=0))[0]\n",
    "    #nonzero_indexes = np.unique(np.nonzero(l1_lr.coef_)[1]) # 2nd method\n",
    "    nonzero_features = np.array(vect.get_feature_names())[nonzero_indexes]\n",
    "    \n",
    "    # create the vectorizer with the reduced vocabulary\n",
    "    rv_vect = CountVectorizer(vocabulary=nonzero_features)    \n",
    "    rv_train_features = rv_vect.fit_transform(train_data)\n",
    "    rv_data_features = rv_vect.transform(dev_data)\n",
    "    \n",
    "    l2_rv_lr = LogisticRegression(C=C, penalty='l2', tol=tol)\n",
    "    l2_rv_lr.fit(rv_train_features, train_labels)\n",
    "    sparsity_l2_rv_lr = np.mean(l2_rv_lr.coef_.ravel() == 0) * 100\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # compare the three models\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    print('C=%.2f' % C)\n",
    "    \n",
    "    def print_output(penalty, coef_shape, num_nonzero_weights, sparsity, score):\n",
    "        print('-' * 80)\n",
    "        print('penalty %s' % penalty)\n",
    "        print('-' * 80)\n",
    "        print('(n_classes, n_features): {}'.format(coef_shape))\n",
    "        print('Number of nonzero weights: %d' % num_nonzero_weights)\n",
    "        print('Sparsity: %.2f%%' % sparsity)\n",
    "        print('Score: %.4f' % score) # score == mean accuracy\n",
    "    \n",
    "    print_output('L1', l1_lr.coef_.shape, np.count_nonzero(l1_lr.coef_), sparsity_l1_lr, l1_lr.score(train_features, train_labels))\n",
    "    print_output('L2', l2_lr.coef_.shape, np.count_nonzero(l2_lr.coef_), sparsity_l2_lr, l2_lr.score(train_features, train_labels))\n",
    "    print_output('L2 with reduced vocabulary', l2_rv_lr.coef_.shape, np.count_nonzero(l2_rv_lr.coef_), sparsity_l2_rv_lr, l2_rv_lr.score(rv_train_features, train_labels))\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "    # plot accuracy of re-trained model vs vocab size \n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('plot accuracy of re-trained model vs vocab size')\n",
    "    print('-' * 80)\n",
    "    \n",
    "    Cs = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]\n",
    "    n_features = []\n",
    "    n_nz_weights = []\n",
    "    sparsities = []\n",
    "    scores = []\n",
    "    \n",
    "    for C in Cs:\n",
    "        # logistic regression with penalty l1\n",
    "        l1_lr = LogisticRegression(C=C, penalty='l1', tol=tol)\n",
    "        l1_lr.fit(train_features, train_labels)    \n",
    "        # reduced vocabulary: keep features that have at least one non-zero weight\n",
    "        nonzero_indexes = np.nonzero(np.any(l1_lr.coef_ != 0, axis=0))[0]\n",
    "        nonzero_features = np.array(vect.get_feature_names())[nonzero_indexes]\n",
    "        # create the vectorizer with the reduced vocabulary\n",
    "        rv_vect = CountVectorizer(vocabulary=nonzero_features)    \n",
    "        rv_train_features = rv_vect.fit_transform(train_data)\n",
    "        rv_data_features = rv_vect.transform(dev_data)\n",
    "        # logistic regression with penalty l2 and reduced vocabulary\n",
    "        l2_rv_lr = LogisticRegression(C=C, penalty='l2', tol=tol)\n",
    "        l2_rv_lr.fit(rv_train_features, train_labels)\n",
    "        # store outputs\n",
    "        n_features.append(l2_rv_lr.coef_.shape[1])\n",
    "        n_nz_weights.append(np.count_nonzero(l2_rv_lr.coef_))\n",
    "        sparsities.append(np.mean(l2_rv_lr.coef_.ravel() == 0) * 100)\n",
    "        scores.append(l2_rv_lr.score(rv_train_features, train_labels))\n",
    "\n",
    "    # print outputs in table format\n",
    "    table_data = [Cs, n_features, n_nz_weights, sparsities, scores]\n",
    "    row_format = '{:<23}' * len(table_data)\n",
    "    print(row_format.format(*['C Value', 'Vocab Size', 'Nonzero Weights', 'Sparsity (%)', 'Score']))\n",
    "    for row in zip(*table_data):\n",
    "        print(row_format.format(*row))  \n",
    "        \n",
    "    # print outputs in graph\n",
    "    plt.plot(n_features, scores, 'bo-')\n",
    "    plt.title('Accuracy vs. Vocabulary Size')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Accuracy (measured on dev set)')\n",
    "    plt.xlim(xmax=9000)\n",
    "    plt.ylim(ymin=0.25)\n",
    "    plt.grid(True)    \n",
    "    \n",
    "    # annotate C-values on plot\n",
    "    for index, C in enumerate(Cs):\n",
    "        plt.annotate('C='+str(C), xy=(n_features[index], scores[index]),\n",
    "                     xytext=(n_features[index]+100, scores[index]-0.02),\n",
    "                     size=8, rotation=-40)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiffapedia/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score to improve in P8: 0.7702\n",
      "--------------------------------------------------------------------------------\n",
      "#1 R: 929.3585\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.993720\n",
      "actual         3  talk.religion.misc     0.001069\n",
      "\n",
      "Message:\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "--------------------------------------------------------------------------------\n",
      "#2 R: 325.0045\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.979746\n",
      "actual         3  talk.religion.misc     0.003015\n",
      "\n",
      "Message:\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "--------------------------------------------------------------------------------\n",
      "#3 R: 287.1790\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      3  talk.religion.misc     0.695462\n",
      "actual         0         alt.atheism     0.002422\n",
      "\n",
      "Message:\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def P7():\n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    # define changing values\n",
    "    C = 100\n",
    "    top_n = 3\n",
    "    \n",
    "    # create the vectorizer that transforms text into TF-IDF matrix (aka features)\n",
    "    # with row = # of messages (or examples), column = # of vocabularies, value = vocab's (or word's) TF-IDF\n",
    "    vect = TfidfVectorizer()\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    dev_features = vect.transform(dev_data)\n",
    "    \n",
    "    # train logistic regression with C = 100\n",
    "    lr_model = LogisticRegression(C=C)\n",
    "    lr_model.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict labels \n",
    "    pred_labels = lr_model.predict(dev_features)\n",
    "    \n",
    "    # calculate the f1-score\n",
    "    print('F1-score to improve in P8: {:.4f}'.format(metrics.f1_score(dev_labels, pred_labels, average='weighted')))\n",
    "    \n",
    "    # determine the top_n documents where R is largest\n",
    "    # R = maximum predicted probability / predicted probability of the correct label\n",
    "    \n",
    "    # 1. find maximum predicted probability\n",
    "    pred_probs = lr_model.predict_proba(dev_features) \n",
    "    # where row = # of vocabularies, column = # of newsgroup, value = probability of word (or vocab)\n",
    "    max_pred_prob = np.amax(pred_probs, axis=1)\n",
    "    # maximum predicted probability of the word (or vocab) across the different (4) newsgroup\n",
    "    \n",
    "    # 2. find predicted probability of the correct label\n",
    "    pred_label_pred_prob = pred_probs[(np.arange(pred_probs.shape[0]), pred_labels)]\n",
    "    actual_label_pred_prob = pred_probs[(np.arange(pred_probs.shape[0]), dev_labels)]\n",
    "    \n",
    "    # 3. compute R\n",
    "    R = max_pred_prob / actual_label_pred_prob\n",
    "    \n",
    "    # 4. list top_n documents where R is largest \n",
    "    for rank, doc_index in enumerate(np.argsort(-R)[:top_n]): \n",
    "        print('-' * 80)\n",
    "        print('#{} R: {:.4f}\\n'.format(rank + 1, R[doc_index]))\n",
    "        print(pd.DataFrame({'label': [pred_labels[doc_index], dev_labels[doc_index]], \n",
    "                            'newsgroup': [newsgroups_test.target_names[pred_labels[doc_index]], newsgroups_test.target_names[dev_labels[doc_index]]],\n",
    "                            'probability': [pred_label_pred_prob[doc_index], actual_label_pred_prob[doc_index]]}, \n",
    "                           columns=['label', 'newsgroup', 'probability'],\n",
    "                           index=['predicted', 'actual']))\n",
    "        print('\\nMessage:\\n%s' % (dev_data[doc_index]))\n",
    "\n",
    "    ### STUDENT END ###\n",
    "P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "How is TfidfVectorizer different from the CountVectorizer?\n",
    "\n",
    "Both TfidfVectorizer and CountVectorizer measures the importance of a word in the document. CountVectorizer measures the importance of a word by counting the number of times the word apears in the document. On the other hand, TfidfVectorizer measures the importance of a word by counting the number of times the word appears in the document and also offsetting the frequency of the word in the corpus. In other words, TfidfVectorizer decreases the weight for commonly used words (such as stop words) and increases the weight for words that are not used very much in a collection of documents. The difference between the two is that both apply the term-frequency (tf) measurement but only TfidfVectorizer uses the inverse document frequency (idf).\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see.\n",
    "\n",
    "1. The vectorizer did not normalize the input to improve generalization, such as removing accents, removing stop words, employing better preprocessor like the one we have defined earlier in P5. \n",
    "2. We can employ n-grams to determine which phrases are used most common in each newsgroup (instead of just words) since confusion between newsgroup such as comp.graphics and talk.religion.misc happens when it should not have happened.\n",
    "3. We should ignore terms that have a document frequency greater than 2 because talk.religion.misc and alt.atheism are the two newsgroups that are alike and the others are quite distinct from one another.\n",
    "4. The regularization term C = 100 might not be optimal as it affords leniency to ovefitting of data. \n",
    "5. We should also normalize the term vectors to prevent overfitting as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model.\n",
    "\n",
    "- [1 pt] for a reasonable attempt\n",
    "- [2 pts] for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiffapedia/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.4358\n",
      "--------------------------------------------------------------------------------\n",
      "#1 R: 1.7765\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.311028\n",
      "actual         3  talk.religion.misc     0.175084\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "Convenient?  It seems very appropriate that this is cross-posted to\n",
      "alt.conspiracy.\n",
      "\n",
      "Assuming the most favorable interpretation of your '1000 degree'\n",
      "measurement (that the temperature is in Centigrade, rather than the\n",
      "more common -in the US- Fahrenheit), you are still laboring under at\n",
      "least 2 misconceptions:\n",
      "\n",
      "1.  You seem to believe that steel melts somewhere around 1000 C.\n",
      "    Actually, the melting point of most iron alloys (and steels are\n",
      "    iron alloys) is in the neighborhood of 1400 C.  Even if the gun\n",
      "    were found in area which achieved the 1000 C temperature, the steel\n",
      "    parts of the gun would not be deformed, and it would still be\n",
      "    trivial to identify the nature of the weapon.\n",
      "\n",
      "2.  A fire is not an isothermal process.  There are 'hot' spots and\n",
      "    'cold' spots, though 'cold' is purely a relative term.   So the\n",
      "    weapon was not necessarily situated in a hot spot, as you seem to\n",
      "    imply.  And, even if it was, so what?  It would not have melted\n",
      "    anyway.\n",
      "--------------------------------------------------------------------------------\n",
      "#2 R: 1.7275\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.312105\n",
      "actual         3  talk.religion.misc     0.180664\n",
      "\n",
      "Message:\n",
      "\n",
      "Well, let me see if I can explain it.  It's similar to collecting coins,\n",
      "or stamps, or campaign buttons, or coke bottles, or juke boxes, or model\n",
      "trains, or just about anything else that is collected (and just about\n",
      "everything is collected).  In all cases, you might consider it something\n",
      "of an aberration; I mean, what purpose does it serve?  Not much really;\n",
      "it's just a hobby.  The collector yearns for diversity (not much use in\n",
      "having TWO of the same thing, except for trading/selling it), historical\n",
      "significance (this was the thingy used by so-and-so), technical significance\n",
      "(this is the only one that does such-and-such like this; this is the first\n",
      "one to do it this way), rarity, and so on.\n",
      "\n",
      "Some people use their collections, other people do not.  As you state, you\n",
      "use your collection.  In one sense, this diminishes the value of your\n",
      "collection as the items suffer wear and exposure.  In another sense, it\n",
      "can enhance your own enjoyment of your collection.  Some people collect\n",
      "firearms that they do not use; other people use some or all of the firearms\n",
      "they collect.  It's just personal preference.\n",
      "\n",
      "Oops, 'personal preference' ... I guess we're not supposed to have that any\n",
      "more, are we?\n",
      "--------------------------------------------------------------------------------\n",
      "#3 R: 1.7025\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.305078\n",
      "actual         3  talk.religion.misc     0.179189\n",
      "\n",
      "Message:\n",
      "Dear netters\n",
      "\n",
      "I am wondering about the accident of Koresh. I have heard different \n",
      "explanations.\n",
      "Without any explanation about your opinions and believes,\n",
      " please kindly tell me:\n",
      "\n",
      "     1)- What was Koresh talking about?. (Or  what was his message)\n",
      "     2)- What was the main reason that Government went in war with\n",
      "             Koresh?\n",
      "(Some say that due to Tax payment, ....)\n",
      "\n",
      "Thanks in advance for your historical explanation.\n",
      "--------------------------------------------------------------------------------\n",
      "#4 R: 1.7018\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.306047\n",
      "actual         3  talk.religion.misc     0.179832\n",
      "\n",
      "Message:\n",
      "The universe, mirrored in a puddle.\n",
      "Isn't it amazing how there *always* seems to be *another* bottle of bheer there?\n",
      "\n",
      "Aleph *one* bottles of beer on the wall, Aleph *one* null bottles of beer!\n",
      "\n",
      "\tyou, too, are a puddle.\n",
      "\tAs above, so below.\n",
      "--------------------------------------------------------------------------------\n",
      "#5 R: 1.6992\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.300334\n",
      "actual         3  talk.religion.misc     0.176750\n",
      "\n",
      "Message:\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "--------------------------------------------------------------------------------\n",
      "#6 R: 1.6694\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.302694\n",
      "actual         3  talk.religion.misc     0.181322\n",
      "\n",
      "Message:\n",
      "nelson_p@apollo.hp.com (Peter Nelson) writes...\n",
      "\n",
      "\n",
      "The rest of the story seems to be that the agreement for the broadcast\n",
      "was for prime-time, and that Koresh never even heard it played. Wasn't\n",
      "even tuned in to the radio when it aired -- so no reason to come out.\n",
      "\n",
      "If later they had given him a copy of the grossly twisted newswire \n",
      "transcript -- I'm certain Koresh would think he was at the mercy of\n",
      "evil itself. \n",
      "\n",
      "As to coming out after Passover, wasn't that just one of the lawyer's\n",
      "speculations Peter?\n",
      "--------------------------------------------------------------------------------\n",
      "#7 R: 1.6610\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.301269\n",
      "actual         3  talk.religion.misc     0.181383\n",
      "\n",
      "Message:\n",
      "On 21-Apr-93 in Re: ABORTION and private he..\n",
      "Not so in PA.  Recently the gender inequity in auto insurance was\n",
      "removed.  Just a point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#8 R: 1.6537\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.299884\n",
      "actual         3  talk.religion.misc     0.181344\n",
      "\n",
      "Message:\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "--------------------------------------------------------------------------------\n",
      "#9 R: 1.6518\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.299705\n",
      "actual         3  talk.religion.misc     0.181442\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "Yeah.  In a fire that reportedly burned hotter than 1000 degrees-- hot\n",
      "enough to make the bodies still unidentifiable-- the authorities found\n",
      "a gun that was recognizably fully-automatic and state of the art.\n",
      "Isn't that CONVEEEENIENT?\n",
      "--------------------------------------------------------------------------------\n",
      "#10 R: 1.6477\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.300476\n",
      "actual         3  talk.religion.misc     0.182358\n",
      "\n",
      "Message:\n",
      "\n",
      "I think we should just let Bhagwans be Bhagwans.\n",
      "--------------------------------------------------------------------------------\n",
      "#11 R: 1.6324\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.295735\n",
      "actual         3  talk.religion.misc     0.181168\n",
      "\n",
      "Message:\n",
      "\n",
      "Faith and intelligence tell me that when a druggie breaks into my house at\n",
      "night with a knife to kill me for the $2 in my wallet, a .357 is considerably\n",
      "more persuasive than having devotions with him.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#12 R: 1.6226\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.296036\n",
      "actual         3  talk.religion.misc     0.182448\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "No.  Zeno's paradox is resolved by showing that integration or an infinite\n",
      "series of decreasing terms can sum to a finite result.\n",
      "\n",
      "\n",
      "Well, suppose a probe emitting radiation at a constant frequency was\n",
      "sent towards a black hole.  As it got closer to the event horizon, the\n",
      "red shift would keep increasing.  The period would get longer and longer,\n",
      "but it would never stop.  An observer would not observe the probe actually\n",
      "reaching the event horizon.  The detected energy from the probe would keep\n",
      "decreasing, but it wouldn't vanish.  Exp(-t) never quite reaches zero.\n",
      "\n",
      "I guess the above probably doesn't make things any more clear, but hopefully\n",
      "you will get the general idea maybe.\n",
      "--------------------------------------------------------------------------------\n",
      "#13 R: 1.6225\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.303045\n",
      "actual         3  talk.religion.misc     0.186776\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Baloney.  Either the programmer or the people who decided to let their\n",
      "actions be governed by the program are clearly at fault.  If you neglect\n",
      "to do maintenance on your car, and the steering goes out, you _are_\n",
      "responsible for the death of all those kids on the sidewalk your car\n",
      "subsequently drives over \"on its own\".\n",
      "--------------------------------------------------------------------------------\n",
      "#14 R: 1.6225\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.294439\n",
      "actual         3  talk.religion.misc     0.181477\n",
      "\n",
      "Message:\n",
      "Please excuse the length of this post, but for personal reasons, \n",
      "I must go on at some length.\n",
      "\n",
      "    [...more deleted...]\n",
      "\n",
      "    Perhaps it would be instructive to see what my original post had\n",
      "to say:\n",
      "\n",
      "     [followed by my signature]\n",
      "\n",
      "    I was extremely careful in this posting not to say anything which\n",
      "was not factual.  I made no judgement about Hitching or the quality of \n",
      "the quotation attributed to him.  I have not read any of the books \n",
      "listed (although I did glimpse briefly at \"Earth Magic\", I saw nothing \n",
      "that I would care to comment on).  It was solely in response to an\n",
      "inquiry by Warren about Hitching, and your assertion that he is a\n",
      "paleontologist.  I do not know whether he is or is not a paleontologist.\n",
      "I do not claim to know anything about him, except this listing of his\n",
      "publications.\n",
      "\n",
      "    However, I get the decided impression that I am being included\n",
      "among the \"Branch Atheists\" on the basis of this post.  If that \n",
      "impression is mistaken, please let me know.  Otherwise, I should let\n",
      "you know that the implications are very offensive to me, and I \n",
      "would certainly appreciate a clarification of your posting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#15 R: 1.6141\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.295622\n",
      "actual         3  talk.religion.misc     0.183148\n",
      "\n",
      "Message:\n",
      "    ...\n",
      "    ...\n",
      "\n",
      "    Interesting idea.\n",
      "\n",
      "    This suggestion has inspired me to post, under the title \"Theories \n",
      "of Creation\", a collection of various \"philosophies\" of creation that \n",
      "I am aware of.  Could you explain which of these theories you would\n",
      "want taught, and which ones you would not?  Or, perhaps, I haven't\n",
      "included a favorite theory of yours (if so, could you describe it for\n",
      "me for inclusion in an updated list)?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#16 R: 1.6137\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.296015\n",
      "actual         3  talk.religion.misc     0.183437\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "And maybe they do.  But without somebody to set the time that doesn't\n",
      "do them any good.\n",
      "\n",
      "\n",
      "Humph.  Deleted there was my list of non-religious reasons one might\n",
      "want a moment of silence for a dead classmate.\n",
      "\n",
      "Maybe everyone doesn't want to be silent for teachers to give their\n",
      "pompous non-religious speeches in assembly.  I know I didn't.  So?\n",
      "\n",
      "\n",
      "\n",
      "Please provide documentation that opposing only things that are\n",
      "actively religious (e.g. actual prayer, \"Amen\" after a moment of\n",
      "silence, mandatory classes in religion) and not things that have\n",
      "possible but uncertain religious implications (e.g. moments of\n",
      "silence, having the Bible on the shelves during reading period) is not\n",
      "a way to prevent a state religion.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#17 R: 1.6122\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.301736\n",
      "actual         3  talk.religion.misc     0.187152\n",
      "\n",
      "Message:\n",
      "[ deleted ]\n",
      "[ deleted ]\n",
      "\n",
      "  Your information on this topic is very much out of date.  Quantum Electro-\n",
      "dynamics (QED - which considers light to be particles) has been experimentally\n",
      "verified to about 14 decimal digits of precision under ALL tested conditions.\n",
      "I'm afraid that this case, at least in the physics community, has been decided.\n",
      "Laymen should consult \"QED - The Strange Theory of Light and Matter\" by Richard\n",
      "P. Feynman and for the more technically minded there's \"The Feynman Lectures on\n",
      "Physics\" by Feynman, Leighton and Sands (an excellent 3 volumes).  Case closed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#18 R: 1.6006\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.295405\n",
      "actual         3  talk.religion.misc     0.184563\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "The problem is that you imagine him inside this huge wall, unable\n",
      "to see reality. While he imagines the same about you. Clearly we\n",
      "have a case where relativity plays a big role concerning looking\n",
      "at opposite frames of reality.\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "--------------------------------------------------------------------------------\n",
      "#19 R: 1.5957\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      2           sci.space     0.293110\n",
      "actual         3  talk.religion.misc     0.183683\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "This is not correct.  The event horizon is not the \"center\" of the black\n",
      "hole but merely the distance at which the escape velocity is equal to the\n",
      "speed of light.  That is, the event horizon is a finite radius...\n",
      "--------------------------------------------------------------------------------\n",
      "#20 R: 1.5951\n",
      "\n",
      "           label           newsgroup  probability\n",
      "predicted      1       comp.graphics     0.292833\n",
      "actual         3  talk.religion.misc     0.183586\n",
      "\n",
      "Message:\n",
      "I wasn't sure if this was the right newsgroup to post this to, but I guess\n",
      "the misc is there for a reason.  Here goes...  I am getting married in June to \n",
      "a devout (Wisconsin Synod) Lutheran.  I would classify myself as a strong \n",
      "agnostic/weak athiest.  This has been a a subject of many discussions between\n",
      "us and is really our only real obstacle.  We don't have any real difficulties \n",
      "with the religious differences yet, but I expect they will pop up when we have \n",
      "children.  I have agreed to raise the\n",
      "children \"nominally\" Lutheran.  That is, Lutheran traditions, but trying to\n",
      "keep an open mind.  I am not sure if this is even possible though.  I feel that\n",
      "that the worst quality of being devoutly religous is the lack of an open mind.\n",
      "\n",
      "     Anyway, I guess I'll get on with my question.  Is anyone in the same \n",
      "situation and can give some suggestions as to how to deal with this?  We've \n",
      "taken the attitude so far of just talking about it a lot and not letting \n",
      "anything get bottled up inside.  Sometimes I get the feeling we're making this \n",
      "much bigger than it actually is.  Any comments would be greatly appreciated.  \n",
      "Also, please e-mail responses since I don't get a chance to read this group\n",
      "often.  :-(\n"
     ]
    }
   ],
   "source": [
    "def P8():\n",
    "    ### STUDENT START ###\n",
    "\n",
    "    # define: \n",
    "    # 1. the number of documents to look at where R is largest\n",
    "    top_n = 20\n",
    "    # 2. logistic regression model: regularization based on the optimal param found in P3 \n",
    "    C = 1\n",
    "    penalty = 'l2'\n",
    "    # 3. TfidfVectorizer: \n",
    "    # ngram_range: determine which words and phrases are used most common\n",
    "    # max_df: ignore terms that have a document frequency strictly higher than specified\n",
    "    ngram_range = (1, 3)\n",
    "    max_df = 3\n",
    "    \n",
    "    # create the vectorizer that transforms text into TF-IDF matrix (aka features)\n",
    "    # with row = # of messages (or examples), column = # of vocabularies, value = vocab's (or word's) TF-IDF\n",
    "    vect = TfidfVectorizer(strip_accents='unicode', \n",
    "                           analyzer='word', \n",
    "                           preprocessor=better_preprocessor, \n",
    "                           ngram_range=ngram_range, \n",
    "                           max_df=max_df, \n",
    "                           stop_words='english', \n",
    "                           lowercase=True, \n",
    "                           norm='l2', \n",
    "                           use_idf=True, \n",
    "                           smooth_idf=True)\n",
    "    train_features = vect.fit_transform(train_data)\n",
    "    dev_features = vect.transform(dev_data)\n",
    "    \n",
    "    # train logistic regression with optimal param found in P3\n",
    "    #lr_model = LogisticRegression(C=C, penalty=penalty)\n",
    "    lr_model = LogisticRegression(C=C)\n",
    "    lr_model.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict labels \n",
    "    pred_labels = lr_model.predict(dev_features)\n",
    "    \n",
    "    # calculate the f1-score\n",
    "    print('F1-score: {:.4f}'.format(metrics.f1_score(dev_labels, pred_labels, average='weighted')))\n",
    "    \n",
    "    # determine the top_n documents where R is largest\n",
    "    # R = maximum predicted probability / predicted probability of the correct label\n",
    "    \n",
    "    # 1. find maximum predicted probability\n",
    "    pred_probs = lr_model.predict_proba(dev_features) \n",
    "    # where row = # of vocabularies, column = # of newsgroup, value = probability of word (or vocab)\n",
    "    max_pred_prob = np.amax(pred_probs, axis=1)\n",
    "    # maximum predicted probability of the word (or vocab) across the different (4) newsgroup\n",
    "    \n",
    "    # 2. find predicted probability of the correct label\n",
    "    pred_label_pred_prob = pred_probs[(np.arange(pred_probs.shape[0]), pred_labels)]\n",
    "    actual_label_pred_prob = pred_probs[(np.arange(pred_probs.shape[0]), dev_labels)]\n",
    "    \n",
    "    # 3. compute R\n",
    "    R = max_pred_prob / actual_label_pred_prob\n",
    "    \n",
    "    # 4. list top_n documents where R is largest \n",
    "    for rank, doc_index in enumerate(np.argsort(-R)[:top_n]): \n",
    "        print('-' * 80)\n",
    "        print('#{} R: {:.4f}\\n'.format(rank + 1, R[doc_index]))\n",
    "        print(pd.DataFrame({'label': [pred_labels[doc_index], dev_labels[doc_index]], \n",
    "                            'newsgroup': [newsgroups_test.target_names[pred_labels[doc_index]], newsgroups_test.target_names[dev_labels[doc_index]]],\n",
    "                            'probability': [pred_label_pred_prob[doc_index], actual_label_pred_prob[doc_index]]}, \n",
    "                           columns=['label', 'newsgroup', 'probability'],\n",
    "                           index=['predicted', 'actual']))\n",
    "        print('\\nMessage:\\n%s' % (dev_data[doc_index]))\n",
    "    ### STUDENT END ###\n",
    "    return\n",
    "\n",
    "P8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that implementing the suggestions in P7 improved the performance significantly at the expense of the F1-score. Extending R to the top 20 documents, I verify the confusion makes practical sense as I myself in a similar situation would not have been able to categorize the message to the correct newsgroup. The model is still better than randomly guessing which message goes to which subgroup because random selection will give us a rough accuracy of 25% while employing the model gives us a rough f1-score of 43.58%. The next step that we can take is to determine the vocabularies most commonly used in the newsgroups and apply those words with heavier weights.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
